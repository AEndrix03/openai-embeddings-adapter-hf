version: "3.9"
services:
  adapter-cpu:
    profiles: ["cpu"]
    build:
      context: .
      dockerfile: Dockerfile.cpu
    ports:
      - "8000:8000"
    env_file:
      - .env.example
    environment:
      ADAPTER_MODEL_DEVICE: cpu
      ADAPTER_CACHE_PATH: /var/cache/adapter/embeddings_cache.sqlite3
    volumes:
      - hf-cache:/hf-cache
      - embeddings-cache:/var/cache/adapter

  adapter-gpu:
    profiles: ["gpu"]
    build:
      context: .
      dockerfile: Dockerfile.gpu
      args:
        TORCH_BASE_IMAGE: ${TORCH_BASE_IMAGE:-rocm/pytorch:rocm6.4_ubuntu24.04_py3.12_pytorch_release_2.4.1}
    ports:
      - "8000:8000"
    env_file:
      - .env.example
    environment:
      ADAPTER_MODEL_DEVICE: rocm #cuda, auto
      ADAPTER_CACHE_PATH: /var/cache/adapter/embeddings_cache.sqlite3
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    volumes:
      - hf-cache:/hf-cache
      - embeddings-cache:/var/cache/adapter

volumes:
  hf-cache:
  embeddings-cache:
